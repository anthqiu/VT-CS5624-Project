{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Files\n",
    "\n",
    "- `./cnn-7.csv`\n",
    "- `./cnn-8.csv`\n",
    "- `./foxnews-transcript-urls-2025.csv`\n",
    "- `./foxnews-html/` *extracted from* `fnc_transcripts_html_2025.tar.gz.part{1-4}`"
   ],
   "id": "c47c1b5bfe0f689e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import pandas as pd",
   "id": "5ac2e06ef558c6a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cnn_7 = pd.read_csv(\"cnn-7.csv\")\n",
    "cnn_8 = pd.read_csv(\"cnn-8.csv\")\n",
    "cnn = pd.concat([cnn_7, cnn_8])\n",
    "del cnn_7\n",
    "del cnn_8\n",
    "cnn.info()"
   ],
   "id": "96a4dfa1bc6af23c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "start_date=pd.Timestamp(\"2015-01-01\", tz=\"UTC\")\n",
    "end_date=pd.Timestamp(\"2025-03-01\", tz=\"UTC\")"
   ],
   "id": "6f13040f09d64809",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cnn[\"timestamp\"] = cnn.apply(lambda x: f\"{x['year']:.0f}-{x['month']:02.0f}-{x['date']:02.0f} {x['time']}\", axis=1)\n",
    "cnn[\"ts\"]=pd.to_datetime(cnn[\"timestamp\"], errors='coerce')\n",
    "cnn[\"ts\"] = cnn[\"ts\"].dt.tz_localize(\"America/New_York\", ambiguous=True).dt.tz_convert(\"UTC\")\n",
    "cnn_cleaned=cnn[[\"ts\", \"subhead\", \"text\"]].rename(columns={\"subhead\":\"head\"})\n",
    "cnn_cleaned=cnn_cleaned[(cnn_cleaned[\"ts\"]>=start_date) & (cnn_cleaned[\"ts\"]<end_date)]\n",
    "cnn_cleaned[\"text\"]=cnn_cleaned[\"head\"]+\".  \"+cnn_cleaned[\"text\"]\n",
    "del cnn_cleaned[\"head\"]\n",
    "cnn_cleaned.describe()"
   ],
   "id": "28d7ca5d27202714",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cnn_cleaned.to_csv(\"cnn.csv\", index=False)",
   "id": "762bb9d99b8e0d91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "del cnn\n",
    "del cnn_cleaned"
   ],
   "id": "7eb11e9dca244c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fox=pd.read_csv(\"foxnews-transcript-urls-2025.csv\")[[\"publicationDate\", \"title\", \"html_file\"]]\n",
    "fox[\"html_file\"]=fox[\"html_file\"].apply(lambda x: x.replace(\".html\", \"\"))\n",
    "fox"
   ],
   "id": "c731bc96dd5dddf6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "INPUT_DIR = \"./foxnews-html\"\n",
    "HTML_DIR = \"./foxnews-html-decompressed\"\n",
    "if not os.path.exists(HTML_DIR):\n",
    "    os.makedirs(HTML_DIR)\n",
    "\n",
    "def extract_gz(filename):\n",
    "    with gzip.open(os.path.join(INPUT_DIR, filename), 'rb') as f_in:\n",
    "        with open(os.path.join(HTML_DIR, filename[:-3]), 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    return True\n",
    "\n",
    "gz_files=[]\n",
    "for root, dirs, files in os.walk(INPUT_DIR):\n",
    "    gz_files = files\n",
    "\n",
    "with Pool(cpu_count()) as pool:\n",
    "    for _ in tqdm(\n",
    "        pool.imap_unordered(extract_gz, gz_files),\n",
    "        total=len(gz_files),\n",
    "        desc=\"Decompressing .gz files\"\n",
    "    ):\n",
    "        pass"
   ],
   "id": "679a4058c26438f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "TEXT_DIR = \"./foxnews-text\"\n",
    "if not os.path.exists(TEXT_DIR):\n",
    "    os.makedirs(TEXT_DIR)\n",
    "\n",
    "def extract_fox_transcript_from_html(html: str) -> str | None:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    for script in soup.find_all(\"script\", type=\"application/ld+json\"):\n",
    "        if not script.string:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            data = json.loads(script.string)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        candidates = []\n",
    "        if isinstance(data, dict):\n",
    "            candidates = [data]\n",
    "        elif isinstance(data, list):\n",
    "            candidates = data\n",
    "\n",
    "        for node in candidates:\n",
    "            if not isinstance(node, dict):\n",
    "                continue\n",
    "            if node.get(\"@type\") == \"NewsArticle\" and \"articleBody\" in node:\n",
    "                text = node[\"articleBody\"]\n",
    "                return \" \".join(text.split())\n",
    "\n",
    "    return None\n",
    "\n",
    "def extract_text(filename):\n",
    "    with open(os.path.join(HTML_DIR, filename), \"rb\") as f:\n",
    "        html = f.read()\n",
    "    text=extract_fox_transcript_from_html(html)\n",
    "    if text:\n",
    "        with open(os.path.join(TEXT_DIR, filename.replace(\".html\", \".txt\")), \"w\") as f:\n",
    "            f.write(text)\n",
    "    return True\n",
    "\n",
    "html_files=[]\n",
    "for root, dirs, files in os.walk(HTML_DIR):\n",
    "    html_files = files\n",
    "\n",
    "with Pool(cpu_count()) as pool:\n",
    "    for _ in tqdm(\n",
    "        pool.imap_unordered(extract_text, html_files),\n",
    "        total=len(html_files),\n",
    "        desc=\"Extracting articleBody from .html files\"\n",
    "    ):\n",
    "        pass"
   ],
   "id": "cd2329c3e19aabc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fox_text=[]\n",
    "for _, __, files in os.walk(\"./foxnews-text\"):\n",
    "    for file in files:\n",
    "        with open(\"./foxnews-text/\"+file, \"r\") as f:\n",
    "            fox_text.append({\n",
    "                \"text\": f.read(),\n",
    "                \"html_file\": file.replace(\".txt\", \"\")\n",
    "            })\n",
    "fox_text = pd.DataFrame(fox_text)\n",
    "fox_text"
   ],
   "id": "6cb93af18a6f3a36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fox_cleaned = pd.merge(fox, fox_text, how=\"inner\", on=\"html_file\")\n",
    "fox_cleaned"
   ],
   "id": "3f0c1460f6766492",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fox_cleaned[\"publicationDate\"]=pd.to_datetime(fox_cleaned[\"publicationDate\"])\n",
    "fox_cleaned=fox_cleaned[(fox_cleaned[\"publicationDate\"]>=start_date) & (fox_cleaned[\"publicationDate\"]<end_date)]\n",
    "fox_cleaned=fox_cleaned[[\"publicationDate\", \"text\"]].rename(columns={\"publicationDate\": \"ts\"})\n",
    "fox_cleaned"
   ],
   "id": "a4234837055eed1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "fox_cleaned.to_csv(\"fox.csv\", index=False)",
   "id": "918fa9640b416926",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Expected cleansed output\n",
    "\n",
    "- `cnn.csv`\n",
    "- `fox.csv`\n"
   ],
   "id": "6c8e3557411459c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
